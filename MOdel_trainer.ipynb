{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1o316QC46eNuik52L8tv_oFEh79fy-slP",
      "authorship_tag": "ABX9TyP2ePTppICJgkuN9DBl5meu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARSHGit45/vidya---GPT/blob/main/MOdel_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kil69HAvvsBQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_datap.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "PtSD6Ra4wDCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for item in data:\n",
        "    prompt = item['prompt']\n",
        "    for output in item['expected_output']:\n",
        "        train_data.append(f\"Prompt: {prompt}\\nQuestion: {output}\")\n"
      ],
      "metadata": {
        "id": "dGvJQl5MwC-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_data.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in train_data:\n",
        "        f.write(f\"{line}\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "d6OBcp-XwC8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "pXxp9r-OwC6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"train_data.txt\",\n",
        "    block_size=128\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyGR77eZwC3u",
        "outputId": "a9787ea2-205d-4263-be35-e819d1bc58d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "Rbixt2dDzes_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "9aQpR4j6zem5",
        "outputId": "25f7505c-007b-4924-9244-494f215acbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2320' max='2320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2320/2320 04:49, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.216900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.167000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.124700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.099900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2320, training_loss=0.14333701627007847, metrics={'train_runtime': 289.1285, 'train_samples_per_second': 16.048, 'train_steps_per_second': 8.024, 'total_flos': 303098757120000.0, 'train_loss': 0.14333701627007847, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./trained_model\")\n",
        "tokenizer.save_pretrained(\"./trained_model\")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./trained_model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./trained_model\")"
      ],
      "metadata": {
        "id": "uZrKr4pSzekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_questions(prompt, num_questions=1):\n",
        "    input_text = f\"Prompt: {prompt}\\nQuestion:\"\n",
        "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Set pad_token_id to eos_token_id to avoid the warning\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=100,\n",
        "        num_return_sequences=num_questions,\n",
        "        num_beams=5,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated outputs\n",
        "    questions = []\n",
        "    for output in outputs:\n",
        "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        # Ensure the response is split correctly and only the question part is extracted\n",
        "        if \"Question:\" in decoded_output:\n",
        "            question_part = decoded_output.split(\"Question:\")[1].strip()\n",
        "            questions.append(question_part)\n",
        "\n",
        "    return questions\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Generate questions based on Effects of Electric Current chapter for 1 marks\"\n",
        "questions = generate_questions(prompt, num_questions=1)\n",
        "for question in questions:\n",
        "    print(question)  # Print the question directly without any heading\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtGkraf6zehn",
        "outputId": "f5634fce-ce58-435f-cab6-f03594c11da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Find odd man out: Voltmeter, ammeter, galvanometer, thermometer.\n",
            "Prompt: Generate questions based on Effects of Electric Current chapter for 1 marks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r trained_model.zip trained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXJkUSz4zefC",
        "outputId": "0fca90ae-0ab9-41cf-c2b6-4fd906e14a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: trained_model/ (stored 0%)\n",
            "  adding: trained_model/generation_config.json (deflated 24%)\n",
            "  adding: trained_model/model.safetensors (deflated 7%)\n",
            "  adding: trained_model/special_tokens_map.json (deflated 74%)\n",
            "  adding: trained_model/vocab.json (deflated 68%)\n",
            "  adding: trained_model/training_args.bin (deflated 52%)\n",
            "  adding: trained_model/merges.txt (deflated 53%)\n",
            "  adding: trained_model/tokenizer_config.json (deflated 54%)\n",
            "  adding: trained_model/config.json (deflated 52%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_questions(prompt, num_questions=1):\n",
        "    input_text = f\"Prompt: {prompt}\\nQuestion:\"\n",
        "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Set pad_token_id to eos_token_id to avoid the warning\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "    outputs = model.generate(inputs, max_length=100, num_return_sequences=num_questions, num_beams=5, pad_token_id=pad_token_id)\n",
        "\n",
        "    # Decode the generated outputs\n",
        "    questions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "    # Truncate to extract only the required question part\n",
        "    questions = [q.split(\"Question:\")[1].strip() if \"Question:\" in q else q for q in questions]\n",
        "\n",
        "    return questions\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Give reference for What is a redox reaction? Explain with one example. question from Chemical Reactions and Equations chapter\"\n",
        "questions = generate_questions(prompt, num_questions=1)\n",
        "for i, question in enumerate(questions):\n",
        "    print(question)  # Print the question directly without any heading"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X2nUAuNzece",
        "outputId": "efa11199-eeeb-48c7-b30f-e1015fcf6fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.shaalaa.com/question-bank-solutions/explain-redox-reaction-give-one-example-oxidation-reduction-and-redox-reactions_1364#ref=qp&id=11926\n",
            "Prompt: Give reference for What is a redox\n"
          ]
        }
      ]
    }
  ]
}